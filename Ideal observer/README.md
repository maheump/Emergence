# Ideal observer

## Scripts and functions

The different observers available are:
* ```Emergence_IO_Bernoulli``` estimates the frequency of A and B.
* ```Emergence_IO_Markov``` estimates the frequency of first-order transitions (A|A, A|B, B|A, and B|B).
* ```Emergence_IO_Chain``` estimates the frequency of transitions of any order.
* ```Emergence_IO_Tree``` detects repetition of pattern of any length up to a given limit.

The full Bayesian ideal observer of the task is implemented in  ```Emergence_IO_FullIO``` and considers there might be a change point in the sequence separating a fully-stochastic part from a regular part that can be described using one of the previous observers.

Toy examples scripts are available for each of there observers.

## Full Bayesian ideal observer of the task

Here is the result of the inference by the full Bayesian ideal observer of the task (the function ```Emergence_IO_FullIO```).
These figures can be reproduced using the script ```Emergence_IO_ToyExampleFullIO```.

### Posterior probability of

We estimate the posterior probability of each possible model:

<a href="https://www.codecogs.com/eqnedit.php?latex=p\left(\mathcal{M}_{i}|y\right)&space;=&space;\frac{p\left(y|\mathcal{M}_{i}\right)&space;\cdot&space;p\left(\mathcal{M}_{i}\right)}{p\left(y\right)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p\left(\mathcal{M}_{i}|y\right)&space;=&space;\frac{p\left(y|\mathcal{M}_{i}\right)&space;\cdot&space;p\left(\mathcal{M}_{i}\right)}{p\left(y\right)}" title="p\left(\mathcal{M}_{i}|y\right) = \frac{p\left(y|\mathcal{M}_{i}\right) \cdot p\left(\mathcal{M}_{i}\right)}{p\left(y\right)}" /></a>

The first group of plots display the posterior distribution over models (and related metrics).

<p align="center">
  <img src="ToyExamples/figs/Emergence_IO_ToyExampleFullIO_fig1.jpeg" width="800" align="middle">
</p>

### Posterior distribution over models' parameters

The second group of plots display the posterior distribution over models' parameters.

<p align="center">
  <img src="ToyExamples/figs/Emergence_IO_ToyExampleFullIO_fig2.jpeg" width="500" align="middle">
</p>

### Posterior distribution over change point's position

The third group of plots display the posterior distribution over change point's position (and related metrics).

<p align="center">
  <img src="ToyExamples/figs/Emergence_IO_ToyExampleFullIO_fig3.jpeg" width="600" align="middle">
</p>

### Expectation and surprise

The fourth group of plots display the expectations regarding the identity of the next observation (and related metrics).

<p align="center">
  <img src="ToyExamples/figs/Emergence_IO_ToyExampleFullIO_fig4.jpeg" width="700" align="middle">
</p>

## Ideal observer

**General principle**
Knowing the structure of the task, it is possible to derive an ideal inference scenario. Such an inference dictates the normative properties that should rule the learning process. Comparing subjects’ behavior against this benchmark inference therefore allows to identify possible signatures of a normative inference in human behavior. Practically speaking, the ideal observer is presented with a sequence $$y$$ of $$K$$ binary observations (noted $$\mathrm{A}$$ and $$\mathrm{B}$$) as input and return the likelihood that each possible hypothesis $$\mathcal{H}_{i}$$ (fully-stochastic, stochastic-to-probabilistic and stochastic-to-deterministic processes) would be the generative process of that sequence using Bayes’ theorem.
$$p\left(\mathcal{H}_{i}|y\right) = \frac{p\left(y|\mathcal{H}_{i}\right) \cdot p\left(\mathcal{H}_{i}\right)}{p\left(y\right)}$$
Where the marginal likelihood of the sequence is defined as $$p\left(y\right) = \sum_{i} p\left(y|\mathcal{H}_{i}\right) \cdot p\left(\mathcal{H}_{i}\right)$$ and the prior probability over models, it is set to be uniform, such that: $$p\left(\mathcal{H}_{i}\right) = \frac{1}{3}$$.
We now successively describe how the likelihood of the sequence under each hypothesis $$p\left(y|\mathcal{H}_{i}\right)$$ is computed. In each and every case it equals the likelihood of each observation of that sequence under that particular hypothesis.
$$p\left(y_{1:K}|\mathcal{H}_{i}\right) = \prod_{k=1}^{K} p\left(y_{k}|\mathcal{H}_{\text{P}}\right)$$
**Likelihood of a sequence under a fully-stochastic hypothesis**
The sequence's likelihood under a fully-stochastic hypothesis $$\mathcal{H}_{\text{S}}$$ is simply the product of each observation being generated by chance, as in unbiased coin tosses. Because the observations are binary, it follows that the likelihood of receiving one by chance, independently of its identity, is $$\frac{1}{2}$$.
$$p\left(y_{1:K}|\mathcal{H}_{\text{S}}\right) = p\left(\mathrm{A}\right)^{N_{y_{1:K}}^{\mathrm{A}}} + p\left(\mathrm{B}\right)^{N_{y_{1:K}}^{\mathrm{B}}} = \left(\frac{1}{2}\right)^{K}$$
Where $$N_{y}^{\mathrm{X}}$$ denotes the number of times $$\mathrm{X}$$ has been observed in $$y$$ (subscripts denote the observation number within a sequence).
**Likelihood of a sequence under a stochastic-to-regular hypothesis**
For the remaining two hypotheses, because of the existence of a change point (the sequence always starts with a fully-stochastic part), sequence's likelihood cannot be derived as easily. One must indeed consider all the $$j_{k}$$ positions the change point may have taken (after the $$1$$st, $$2$$nd, ... up to the $$K-1$$th observation), and marginalise over these positions as follows.
$$p\left(y_{1:K}|\mathcal{H}_{\mathrm{S}\rightarrow i}\right) = \sum_{k=1}^{K-1} p\left(y_{1:K}|j_{k},\mathcal{H}_{\mathrm{S}\rightarrow i}\right) \cdot p\left(j_{k}\right)$$
$$p\left(y_{1:K}|j_{k},\mathcal{H}_{\mathrm{S}\rightarrow i}\right) = p\left(y_{1:k}|\mathcal{H}_{\mathrm{S}}\right) \cdot p\left(y_{k+1:K}|\mathcal{H}_{i}\right)$$
The likelihood of the first part of the sequence is simply the likelihood of a fully-stochastic sequence (see above) while the likelihood of the second part depends upon the type of regularity that is considered (either probabilistic or deterministic, see below).
The prior distributions over change point's position is set to be uniform, such that all positions are a priori equally likely: $$p\left(j_{k}\right) = \frac{1}{N}$$ where, $$N$$, the length of the entire sequence equals 200 in the experiment (see ***Alternative ideal observers*** for different prior distribution over change point’s position).
**Likelihood of a sequence under a probabilistic hypothesis**
The probabilistic regularities featured in the experimental design have been constrained in the space of transition probabilities. Accordingly, the stochastic-to-probabilistic hypothesis $$\mathcal{H}_{\mathrm{S}\rightarrow \mathrm{P}}$$ estimates a matrix of transition probabilities (Meyniel, Maheu & Dehaene, 2016; see ***Alternative ideal observers*** for different learnt statistics) from the part of the sequence considered as regular (i.e. post change point). The likelihood of that second regular part under an probabilistic hypothesis is computed by integrating over all possible values of inferred transition probabilities $$\theta$$.
$$p\left(y_{k+1:K}|\mathcal{H}_{\text{P}}\right) = \int p\left(y_{k+1:K}|\theta,\mathcal{H}_{\text{P}}\right) \cdot p\left(\theta|\mathcal{H}_{\text{P}}\right) \mathrm{d}\theta$$
For a model that estimates transition probabilities between consecutive stimuli, the likelihood of a given observation depends only on the estimated transition probabilities and the previous stimulus.
$$p\left(y_{k+1:K}|\theta,\mathcal{H}_{\text{P}}\right) = p\left(y_{k+1}|\theta\right) \cdot \prod_{i=k+2}^{K} p\left(y_{i}|y_{i-1},\theta,\mathcal{H}_{\text{P}}\right)$$
First, and for simplicity, the first observation of the second part $$y_{k+1}$$ can be considered as arbitrary which means that its likelihood is $$\frac{1}{2}$$.  Second, the likelihood of the other observations depends upon the inferred value of transition probabilities $$\theta$$ which follow Beta distributions whose parameters equal the counts plus one (which corresponds to the joint uniform prior over transitions’ frequency).
$$p\left(\theta^{\mathrm{A}|\mathrm{B}}|y_{k+1:K}\right) \sim \mathrm{Beta}\left(\theta^{\mathrm{A}|\mathrm{B}}|N_{y_{k+1:K}}^{\mathrm{A}|\mathrm{B}}+1, N_{y_{k+1:K}}^{\mathrm{B}|\mathrm{B}}+1\right)$$
$$p\left(\theta^{\mathrm{B}|\mathrm{A}}|y_{k+1:K}\right) \sim \mathrm{Beta}\left(\theta^{\mathrm{B}|\mathrm{A}}|N_{y_{k+1:K}}^{\mathrm{B}|\mathrm{A}}+1, N_{y_{k+1:K}}^{\mathrm{A}|\mathrm{A}}+1\right)$$
Because of these two points, the sequence likelihood under the probabilistic hypothesis can be entirely rewritten using analytical solutions that simply express the integral of joint Beta distributions as a function of counts (Gelman et al., 2003).
$$p\left(y_{k+1:K}|\mathcal{H}_{\text{P}}\right) = \frac{1}{2} \cdot \prod_{\text{X}\in\{\text{A},\text{B}\}} \frac{\prod_{\text{Y}\in\{\text{A},\text{B}\}} \Gamma \left(N^{\text{Y}|\text{X}}+1\right)}{\Gamma \left(\sum_{\text{Y} = \{\text{A},\text{B}\}} N^{\text{Y}|\text{X}}+1\right)}$$
Where $$\Gamma$$ refers to the gamma function such that $$\Gamma(N)=\left(N-1\right)!$$
**Likelihood of a sequence under a deterministic hypothesis**
The deterministic regularities featured in the experimental design have been defined as the repetition of hand-picked patterns. Accordingly, the stochastic-to-deterministic hypothesis $$\mathcal{H}_{\mathrm{S}\rightarrow \mathrm{D}}$$ tries to explain the part of the sequence considered as regular (i.e. post change point) as the repetition of a particular pattern. The likelihood of that second regular part under such a deterministic hypothesis is obtained by marginalizing over all the possible patterns $$\{\mathcal{R}\}$$ considered under that hypothesis.
$$p\left(y_{k+1:K}|\mathcal{H}_{\text{D}}\right) = \sum_{r\in\{\mathcal{R}\}} p\left(y_{k+1:K}|r,\mathcal{H}_{\text{D}}\right) \cdot p\left(r|\mathcal{H}_{\text{D}}\right)$$
We consider that the set of all patterns $$\{\mathcal{R}\}$$ can be split into two smaller sets. On the one hand, the first one entails all patterns that have already been entirely observed $$\{\mathcal{R}^{o}\}$$ because their length is smaller or equal to the length of the second part of the sequence. On the other hand, the second set entails all patterns that have been only partially observed $$\{\mathcal{R}^{u}\}$$ because their length exceeds the length of the second part of the sequence. Note that both sets depend upon the maximum pattern length allowed, which is denoted as $$\nu$$. We defined $$\nu = 10$$ such that the ideal observer can detect all the patterns we have used in the experiment while being psychologically plausible given human memory limitation (see ***Alternative ideal observers*** for larger possible patterns’ length).
The sequence likelihood can then be rewritten to make these two pattens’ sets explicit as follows.
$$p\left(y_{k+1:K}|\mathcal{H}_{\text{D}}\right) = \sum_{r\in\{\mathcal{R^{o}}\}} p\left(y_{k+1:K}|r,\mathcal{H}_{\text{D}}\right) \cdot p\left(r|\mathcal{H}_{\text{D}}\right) + \sum_{r\in\{\mathcal{R}^{u}\}} p\left(y_{k+1:K}|r,\mathcal{H}_{\text{D}}\right) \cdot p\left(r|\mathcal{H}_{\text{D}}\right)$$
The likelihood of the sequence under a particular pattern $$p\left(y|r\right)$$ can take one of two values: 1 if the pattern $$r$$ can reproduce the sequence $$y$$ and 0 if it cannot. Note however that for partially observed patterns, the likelihood always equal 1 because all partially observed patterns are valid. The prior probability of a pattern $$r$$ depends on its length $$|r|$$ such that: $$p\left(r\right)=\frac{1}{3^{|r|}}$$.
$$p\left(y_{k+1:K}|\mathcal{H}_{\text{D}}\right) = \frac{1}{2} \cdot \left(\sum_{i=1}^{\min\left(K,\nu\right)} (y_{k+1:K} \equiv [y_{k+1:k+i}]^{n}) \cdot \frac{1}{3^{i}} + \sum_{i=K+1}^{\nu} 2^{i-K} \cdot \frac{1}{3^{i}}\right)$$
Note that both sums are multiplied by $$\frac{1}{2}$$ which reflect the fact that the first element of each pattern can either be an $$\mathrm{A}$$ or a $$\mathrm{B}$$ with chance probability. For entirely observed patterns, there is one possible pattern per pattern length
**Posterior distribution of change point’s position**
Besides computing the posterior probability of the three different generative processes given an input sequence, it is also possible to derive posterior beliefs regarding the position of the change point for the two hypotheses that assume the existence of a change point.
$$\forall j_{k}\in\{1,2,\ldots,N\}: p\left(j_{k}|y_{1:K},\mathcal{H}_{\mathrm{S}\rightarrow i}\right) = \frac{p\left(y_{1:K}|j_{k},\mathcal{H}_{\text{S}\rightarrow i}\right) \cdot p\left(j_{k}\right)}{\sum_{k=1}^{N} p\left(y_{1:K}|j_{k},\mathcal{H}_{\text{S}\rightarrow i}\right) \cdot p\left(j_{k}\right)}$$
For already observed change point’s positions, we have defined sequence likelihood before as the product of the likelihood of the first part of the sequence under a fully-stochastic hypothesis and the likelihood of the second part of the sequence under a regular hypothesis.
$$\forall j_{k}\in\{1,2,\ldots,K\}: p\left(y_{1:K}|j_{k},\mathcal{H}_{\mathrm{S}\rightarrow i}\right) = p\left(y_{1:k}|\mathcal{H}_{\mathrm{S}}\right) \cdot p\left(y_{k+1:K}|\mathcal{H}_{i}\right)$$
For yet unobserved change point’s positions, the sequence likelihood is simply the likelihood of that sequence, which is considered to be the first part (the second part having not yet begun), under a fully-stochastic process.
$$\forall j_{k} \in\{K+1,K+2,\ldots,N\}: p\left(y_{1:K}|j_{k},\mathcal{H}_{\mathrm{S}\rightarrow i}\right) = p\left(y_{1:K}|\mathcal{H}_{\mathrm{S}}\right) = \left(\frac{1}{2}\right)^{K}$$
It follows that, even though we have used a non-informative uniform prior over change point’s position $$p(j_{k})=\frac{1}{N}$$, as the sequence unfolds in time it becomes more and more likely that the change point has happened.
Finally, those hypothesis-specific posterior beliefs can then be combined by the means of Bayesian model averaging (BMA) in order to draw inference about the most likely position of the change point independently of the generative hypothesis.
$$p\left(j_{k}|y\right) = \sum_{i \in \{\text{P,D}\}} p\left(j_{k}|y,\mathcal{H}_{\text{S}\rightarrow i}\right) \cdot p\left(\mathcal{H}_{\text{S}\rightarrow i}|y\right)$$
